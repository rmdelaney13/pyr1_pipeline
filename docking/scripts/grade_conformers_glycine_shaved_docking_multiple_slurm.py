#!/usr/bin/env python
import argparse
import json
import os
import shutil
import sys
import time

import pandas as pd
from configparser import ConfigParser

import docking_pipeline_utils as dpu


def csv_to_df_pkl(csv_file_name, pkl_file_name, auto, path_to_conformers, pose, target_res, lig_res_num):
    """
    Converts CSV generated by create_table.py into a DataFrame expected by conformer_prep.
    """
    df = pd.read_csv(csv_file_name, index_col=False)
    if pkl_file_name is None:
        pkl_file_name = f"{csv_file_name[:-4]}.pkl"

    df["Molecule File Stem"] = df["Molecule ID"].apply(lambda name: f"{name}/{name}")
    df["Conformer Range"] = df["Conformer Range"].apply(lambda name: tuple(name.split("_")[:2]))

    if auto:
        print("Auto Generating Alignments")
        for i, row in df.iterrows():
            print(i, end=" ")
            lig = Pose()
            mol_id = row["Molecule ID"]
            conf_num = 1
            res_set = pyrosetta.generate_nonstandard_residue_set(
                lig, params_list=[f"{path_to_conformers}/{mol_id}/{mol_id}.params"]
            )
            pyrosetta.pose_from_file(
                lig, res_set, f"{path_to_conformers}/{mol_id}/{mol_id}_{conf_num:04}.pdb"
            )
            molecule_atoms, target_atoms = alignment.auto_align_residue_to_residue(
                lig, lig.residue(lig_res_num), target_res
            )
            df.loc[i, "Molecule Atoms"] = "-".join(molecule_atoms)
            df.loc[i, "Target Atoms"] = "-".join(target_atoms)

    def split_alabels(name):
        if pd.isna(name):
            return None
        value = str(name).strip()
        if value == "":
            return None
        if value == "default":
            return ("CD2", "CZ2", "CZ3")
        return tuple(value.split("-"))

    df["Molecule Atoms"] = df["Molecule Atoms"].apply(split_alabels)
    df["Target Atoms"] = df["Target Atoms"].apply(split_alabels)

    before_rows = len(df)
    df = df[df["Molecule Atoms"].notnull() & df["Target Atoms"].notnull()].copy()
    if len(df) != before_rows:
        print(
            f"Dropped {before_rows - len(df)} rows from CSV due to missing Molecule/Target Atoms."
        )

    df.to_pickle(pkl_file_name, protocol=4)


def _find_existing_cluster(coords, clusters, rmsd_cutoff):
    for idx, cluster in enumerate(clusters):
        rmsd = dpu.ligand_rmsd(coords, cluster["coords"])
        if rmsd <= rmsd_cutoff:
            return idx
    return None


def align_to_residue_and_check_collision(
    pose,
    res,
    path_to_conformers,
    df,
    pkl_file,
    jump_num,
    rotation,
    translation,
    upper_water_distance,
    lower_water_distance,
    backbone_clash_keep_sidechains,
    max_pass_score=-300,
    bin_width=1,
    vdw_modifier=0.7,
    include_sc=False,
    lig_res_num=1,
    output_dirs=None,
    rename_water_to_tp3=True,
    use_hbond_geometry_filter=True,
    hbond_distance_min=1.5,
    hbond_distance_max=3.5,
    hbond_distance_ideal=2.8,
    hbond_donor_angle_min=100.0,
    hbond_acceptor_angle_min=90.0,
    hbond_quality_min=0.25,
    cluster_enabled=True,
    cluster_rmsd_cutoff=0.75,
):
    """
    Aligns conformers, checks collision, applies H-bond quality filters, and writes clustered outputs.
    """
    import pyrosetta.rosetta.protocols.grafting as graft
    import pyrosetta.rosetta.protocols.rigid as rigid_moves
    from pyrosetta.rosetta.core.pack.task import TaskFactory, operation
    from pyrosetta.rosetta.protocols import minimization_packing as pack_min

    pmm = pyrosetta.PyMOLMover()
    pmm.keep_history(True)

    tf = TaskFactory()
    tf.push_back(operation.InitializeFromCommandline())
    tf.push_back(operation.IncludeCurrent())
    tf.push_back(operation.NoRepackDisulfides())
    tf.push_back(operation.RestrictToRepacking())
    packer = pack_min.PackRotamersMover()
    packer.task_factory(tf)

    sf_all = pyrosetta.get_fa_scorefxn()

    t0 = time.time()
    all_accepted = []
    accepted_conformations = []
    total_confs = 0
    total_pass_score = 0
    total_cluster_kept = 0

    alignto_pose = pose
    clusters = []

    loop_include_sidechain_pose = pose.clone()
    keep_sidechain = [int(x) for x in backbone_clash_keep_sidechains]
    for i in range(1, loop_include_sidechain_pose.total_residue() + 1):
        if i not in keep_sidechain:
            pyrosetta.toolbox.mutate_residue(loop_include_sidechain_pose, i, "G")

    backbone_grid = collision_check.CollisionGrid(
        loop_include_sidechain_pose,
        bin_width=bin_width,
        vdw_modifier=vdw_modifier,
        include_sc=True,
    )

    for pose_info in conformer_prep.yield_ligand_poses(
        df=df,
        path_to_conformers=path_to_conformers,
        post_accepted_conformers=False,
        ligand_residue=lig_res_num,
    ):
        if not pose_info:
            print("Conformer info missing")
            all_accepted.append(accepted_conformations)
            accepted_conformations = []
            continue

        conf = pose_info
        conf.align_to_target(res)

        location = len(alignto_pose.chain_sequence(1))
        new_pose = graft.insert_pose_into_pose(alignto_pose, conf.pose, location)
        ligand_res_index = location + 1

        keep_candidate = False
        copy_pose = None

        count = 0
        while not keep_candidate and count < 30:
            copy_pose = new_pose.clone()
            pert_mover = rigid_moves.RigidBodyPerturbMover(jump_num, rotation, translation)
            pert_mover.apply(copy_pose)
            for atom_id in range(1, copy_pose.residue(ligand_res_index).natoms() + 1):
                atom_coords = copy_pose.residue(ligand_res_index).xyz(atom_id)
                conf.pose.residue(1).set_xyz(atom_id, atom_coords)

            does_collide = conf.check_collision(backbone_grid)
            if not does_collide:
                acceptor_name = None
                neighbor_names = []
                if conf.lig_aid:
                    acceptor_name = conf.lig_aid[0]
                    neighbor_names = list(conf.lig_aid[1:3])

                hbond_result = dpu.evaluate_hbond_geometry(
                    copy_pose,
                    ligand_res_index,
                    acceptor_atom_name=acceptor_name,
                    neighbor_atom_names=neighbor_names,
                    distance_min=hbond_distance_min if use_hbond_geometry_filter else lower_water_distance,
                    distance_max=hbond_distance_max if use_hbond_geometry_filter else upper_water_distance,
                    distance_ideal=hbond_distance_ideal,
                    donor_angle_min=hbond_donor_angle_min,
                    acceptor_angle_min=hbond_acceptor_angle_min,
                    quality_min=hbond_quality_min if use_hbond_geometry_filter else 0.0,
                )
                keep_candidate = hbond_result["passed"] if use_hbond_geometry_filter else (
                    hbond_result["distance"] is not None
                )
            count += 1

        if not keep_candidate or copy_pose is None:
            print("Conformer failed backbone clash and H-bond geometry checks; not saving.")
            total_confs += 1
            continue

        if output_dirs:
            pass_idx = output_dirs["current_pass"]
            array_index = output_dirs["array_index"]
            rotated_dir = output_dirs["rotated_dirs"][pass_idx]
            rotated_path = os.path.join(rotated_dir, f"rotated_{array_index}_{total_confs}.pdb")
        else:
            rotated_path = os.path.join("rotated", f"rotated_{total_confs}.pdb")
        dpu.dump_pose_pdb(copy_pose, rotated_path, rename_water=rename_water_to_tp3)

        packer.apply(copy_pose)
        for atom_id in range(1, copy_pose.residue(ligand_res_index).natoms() + 1):
            atom_coords = copy_pose.residue(ligand_res_index).xyz(atom_id)
            conf.pose.residue(1).set_xyz(atom_id, atom_coords)

        remove_ligand_pose = copy_pose.clone()
        remove_ligand_pose.delete_residue_slow(ligand_res_index)
        grid = collision_check.CollisionGrid(
            remove_ligand_pose,
            bin_width=bin_width,
            vdw_modifier=vdw_modifier,
            include_sc=include_sc,
        )
        does_collide = conf.check_collision(grid)
        if does_collide:
            print("Conformer failed final collision check; not saving.")
            total_confs += 1
            continue

        score = sf_all(copy_pose)
        print(f"Score: {score:.3f}")
        if score >= max_pass_score:
            print("Score too high; not saving this conformer.")
            total_confs += 1
            continue

        total_pass_score += 1
        conf.pose.pdb_info().name("pass_score")
        pmm.apply(conf.pose)

        coords = dpu.ligand_heavy_atom_coords(copy_pose, ligand_res_index)
        if cluster_enabled:
            cluster_idx = _find_existing_cluster(coords, clusters, cluster_rmsd_cutoff)
        else:
            cluster_idx = None

        if cluster_idx is None:
            cluster_id = len(clusters) + 1
            if output_dirs:
                pass_idx = output_dirs["current_pass"]
                array_index = output_dirs["array_index"]
                repacked_dir = output_dirs["pass_score_repacked_dirs"][pass_idx]
                out_path = os.path.join(
                    repacked_dir, f"repacked_{array_index}_cluster{cluster_id:04}.pdb"
                )
            else:
                out_path = os.path.join("pass_score_repacked", f"repacked_cluster{cluster_id:04}.pdb")
            dpu.dump_pose_pdb(copy_pose, out_path, rename_water=rename_water_to_tp3)
            clusters.append({"coords": coords, "score": score, "path": out_path, "conf_num": conf.conf_num})
            accepted_conformations.append(conf.conf_num)
            total_cluster_kept += 1
        else:
            old = clusters[cluster_idx]
            if score < old["score"]:
                dpu.dump_pose_pdb(copy_pose, old["path"], rename_water=rename_water_to_tp3)
                old["coords"] = coords
                old["score"] = score
                old["conf_num"] = conf.conf_num
                accepted_conformations.append(conf.conf_num)
                print(f"Cluster {cluster_idx + 1}: replaced representative with improved score.")
            else:
                print(f"Cluster {cluster_idx + 1}: skipped near-duplicate pose.")

        total_confs += 1

    print(f"\n\n---Output, {pkl_file}---")
    print(f"\nNumber of Ligands: {len(all_accepted)}")
    ligands_accepted = len([e for e in all_accepted if e])
    print(f"Number of Ligands Accepted: {ligands_accepted}")
    print(f"Proportion of Ligands Accepted: {ligands_accepted/len(all_accepted) if all_accepted else 0}")
    total_accepted = sum([len(e) for e in all_accepted])
    print(f"\nNumber of Conformers: {total_confs}")
    print(f"Number of Conformers Accepted: {total_accepted}")
    print(f"Conformers that passed score cutoff: {total_pass_score}")
    print(f"Cluster representatives kept: {total_cluster_kept if cluster_enabled else 'clustering disabled'}")
    print(f"Proportion of Conformers Accepted: {total_accepted/total_confs if total_confs else 0}")
    tf_end = time.time()
    print(f"\nTime taken: {(tf_end - t0)/60:.2f} minutes")
    print(f"Conformers per minute: {total_confs/(tf_end - t0)*60 if (tf_end - t0) > 0 else 0:.2f}")
    df["Accepted Conformers"] = all_accepted
    df.to_pickle(pkl_file, protocol=4)
    return {
        "total_conformers": total_confs,
        "accepted_conformers": total_accepted,
        "pass_score_conformers": total_pass_score,
        "cluster_representatives": total_cluster_kept if cluster_enabled else total_pass_score,
        "cluster_enabled": bool(cluster_enabled),
    }


def aggregate_pass_score_repacked(
    pass_score_repacked_dirs, num_passes, output_base=None, final_dir_name="final_pass_score_repacked"
):
    """
    Aggregates all pass_score_repacked directories into one final directory.
    """
    if output_base:
        parent = os.path.dirname(output_base.rstrip("/"))
        final_dir = os.path.join(parent, final_dir_name)
    else:
        final_dir = os.path.join(os.getcwd(), final_dir_name)

    os.makedirs(final_dir, exist_ok=True)
    print(f"\nCreated final aggregation directory: {final_dir}")

    for pass_idx, source_dir in enumerate(pass_score_repacked_dirs, start=1):
        if not os.path.isdir(source_dir):
            print(f"Source directory {source_dir} does not exist. Skipping.")
            continue

        for file_name in os.listdir(source_dir):
            if file_name.startswith("repacked") and file_name.endswith(".pdb"):
                src_path = os.path.join(source_dir, file_name)
                new_file_name = f"pass{pass_idx}_{file_name}"
                dest_path = os.path.join(final_dir, new_file_name)
                try:
                    shutil.copyfile(src_path, dest_path)
                    print(f"Copied {src_path} to {dest_path}")
                except Exception as e:
                    print(f"Failed to copy {src_path} to {dest_path}: {e}")

    print(f"All pass_score_repacked files have been aggregated into {final_dir}")
    return final_dir


def write_summary_report(output_base, array_index, mode, pass_summaries, final_dir):
    final_unique_clustered_docks = 0
    if os.path.isdir(final_dir):
        final_unique_clustered_docks = len(
            [
                name
                for name in os.listdir(final_dir)
                if name.startswith("pass") and name.endswith(".pdb")
            ]
        )

    report = {
        "mode": mode,
        "array_index": array_index,
        "num_passes": len(pass_summaries),
        "per_pass": pass_summaries,
        "totals": {
            "total_conformers": sum(item["total_conformers"] for item in pass_summaries),
            "accepted_conformers": sum(item["accepted_conformers"] for item in pass_summaries),
            "pass_score_conformers": sum(item["pass_score_conformers"] for item in pass_summaries),
            "cluster_representatives": sum(
                item["cluster_representatives"] for item in pass_summaries
            ),
            "final_unique_clustered_docks": final_unique_clustered_docks,
        },
        "final_clustered_docks_dir": final_dir,
    }

    report_path = os.path.join(output_base, f"summary_array{array_index}.json")
    with open(report_path, "w", encoding="utf-8") as handle:
        json.dump(report, handle, indent=2)

    print(f"\nSummary report written: {report_path}")
    print(
        "Final unique clustered docks (water-geometry + score + clustering): "
        f"{report['totals']['final_unique_clustered_docks']}"
    )
    return report_path


def main(argv):
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("config_file", help="Your config file", default="config.ini", nargs="?")
    parser.add_argument(
        "array_index",
        nargs="?",
        default=0,
        type=int,
        help="SLURM array index (default=0)",
    )
    parser.add_argument(
        "-c",
        "--passes_collision_check",
        help="Whether to only show ones that pass collision check (True/False)",
        default="False",
    )
    if len(argv) == 0:
        parser.print_help()
        return
    args = parser.parse_args(argv)
    array_index = args.array_index

    config = ConfigParser()
    with open(args.config_file, "r", encoding="utf-8-sig") as handle:
        config.read_file(handle)
    default = config["DEFAULT"]
    spec = config["grade_conformers"]
    multiple = config["MULTIPLE_PASSES"] if "MULTIPLE_PASSES" in config else {}

    script_dir = os.path.dirname(os.path.abspath(__file__))
    legacy_dir = os.path.normpath(os.path.join(script_dir, "..", "legacy"))
    for path in [script_dir, legacy_dir]:
        if path not in sys.path:
            sys.path.insert(0, path)

    pyro_path = default.get("PathToPyRosetta", "").strip()
    if pyro_path:
        sys.path.append(pyro_path)
    auto = default.getboolean("AutoGenerateAlignment")

    global pyrosetta, Pose, alignment, conformer_prep, collision_check
    import pyrosetta
    from pyrosetta.rosetta.core.pose import Pose
    import alignment
    import conformer_prep
    import collision_check

    pyrosetta.init("-mute all")
    params_list = default["ParamsList"]

    print("Reading in Pre and Post PDBs")
    if params_list.strip():
        pre_pose = Pose()
        res_set = pyrosetta.generate_nonstandard_residue_set(
            pre_pose, params_list=params_list.split(" ")
        )
        pyrosetta.pose_from_file(pre_pose, res_set, default["PrePDBFileName"])
        post_pose = Pose()
        res_set = pyrosetta.generate_nonstandard_residue_set(
            post_pose, params_list=params_list.split(" ")
        )
        pyrosetta.pose_from_file(post_pose, res_set, default["PostPDBFileName"])
    else:
        pre_pose = pyrosetta.pose_from_pdb(default["PrePDBFileName"])
        post_pose = pyrosetta.pose_from_pdb(default["PostPDBFileName"])

    positions = spec["GlycineShavePositions"].split()
    pocket_shave_positions = [int(x) for x in positions]
    for i in range(1, post_pose.total_residue() + 1):
        if i in pocket_shave_positions:
            pyrosetta.toolbox.mutate_residue(post_pose, i, "G")

    rename_water_to_tp3 = dpu.cfg_getbool(config, "grade_conformers", "RenameWaterToTP3", True)
    dpu.dump_pose_pdb(post_pose, "post_mutate.pdb", rename_water=rename_water_to_tp3)

    target_res_num = int(default["ResidueNumber"])
    chain_letter = default["ChainLetter"]
    pdb2pose = pre_pose.pdb_info().pdb2pose(chain_letter, target_res_num)
    res = pre_pose.residue(pdb2pose)

    path_to_conformers = dpu.cfg_get(config, "create_table", "PathToConformers", default["PathToConformers"])
    csv_file_name = dpu.cfg_get(config, "create_table", "CSVFileName", default["CSVFileName"])
    pkl_file_name = dpu.cfg_get(config, "create_table", "PKLFileName", default["PKLFileName"])
    if not str(pkl_file_name).strip():
        pkl_file_name = None

    auto_prepare = dpu.cfg_getbool(config, "create_table", "AutoPrepareFromSDF", True)
    if auto_prepare:
        dpu.ensure_table_ready(args.config_file, csv_file_name)

    print(f"PKL File Name: {pkl_file_name}")
    bin_width = float(spec["BinWidth"])
    vdw_modifier = float(spec["VDW_Modifier"])
    include_sc = spec.getboolean("IncludeSC")
    lig_res_num = int(default["LigandResidueNumber"])
    jump_num = int(spec["JumpNum"])
    rotation = float(spec["Rotation"])
    translation = float(spec["Translation"])
    upper_water_distance = float(spec["UpperWaterDistance"])
    lower_water_distance = float(spec["LowerWaterDistance"])
    backbone_clash_keep_sidechains = spec["BackboneClashKeepSidechains"].split()
    max_pass_score = float(spec["MaxScore"])

    use_hbond_geometry_filter = dpu.cfg_getbool(
        config, "grade_conformers", "EnableHBondGeometryFilter", True
    )
    hbond_distance_min = float(
        dpu.cfg_get(config, "grade_conformers", "HBondDistanceMin", str(lower_water_distance))
    )
    hbond_distance_max = float(
        dpu.cfg_get(config, "grade_conformers", "HBondDistanceMax", str(upper_water_distance))
    )
    hbond_distance_ideal = float(
        dpu.cfg_get(config, "grade_conformers", "HBondDistanceIdeal", "2.8")
    )
    hbond_donor_angle_min = float(
        dpu.cfg_get(config, "grade_conformers", "HBondDonorAngleMin", "100")
    )
    hbond_acceptor_angle_min = float(
        dpu.cfg_get(config, "grade_conformers", "HBondAcceptorAngleMin", "90")
    )
    hbond_quality_min = float(
        dpu.cfg_get(config, "grade_conformers", "HBondQualityMin", "0.25")
    )

    cluster_enabled = dpu.cfg_getbool(config, "grade_conformers", "EnablePoseClustering", True)
    cluster_rmsd_cutoff = float(
        dpu.cfg_get(config, "grade_conformers", "ClusterRMSDCutoff", "0.75")
    )

    print("Attempting to read .pkl file")
    try:
        df = pd.read_pickle(pkl_file_name)
        print(f"Successfully loaded {pkl_file_name}")
    except FileNotFoundError:
        print(f".pkl file {pkl_file_name} not found, generating one instead")
        csv_to_df_pkl(csv_file_name, pkl_file_name, auto, path_to_conformers, pre_pose, res, lig_res_num)
        df = pd.read_pickle(pkl_file_name)
        print(f"Generated and loaded {pkl_file_name}")

    num_passes = multiple.getint("NumPasses", fallback=1)
    output_base = multiple.get("OutputDirBase", fallback=os.getcwd())
    os.makedirs(output_base, exist_ok=True)

    rotated_dirs = []
    pass_score_repacked_dirs = []

    for pass_idx in range(1, num_passes + 1):
        rotated_dir = os.path.join(output_base, f"rotated_{array_index}_{pass_idx}")
        pass_score_repacked_dir = os.path.join(
            output_base, f"pass_score_repacked_{array_index}_{pass_idx}"
        )
        os.makedirs(rotated_dir, exist_ok=True)
        os.makedirs(pass_score_repacked_dir, exist_ok=True)
        rotated_dirs.append(rotated_dir)
        pass_score_repacked_dirs.append(pass_score_repacked_dir)

    output_dirs = {
        "current_pass": None,
        "rotated_dirs": rotated_dirs,
        "pass_score_repacked_dirs": pass_score_repacked_dirs,
        "array_index": array_index,
    }

    combined_df = pd.DataFrame()
    pass_summaries = []
    for pass_idx in range(1, num_passes + 1):
        print(f"\n--- Starting Pass {pass_idx} ---")
        output_dirs["current_pass"] = pass_idx - 1
        pass_stats = align_to_residue_and_check_collision(
            pose=post_pose,
            res=res,
            path_to_conformers=path_to_conformers,
            df=df,
            pkl_file=pkl_file_name,
            jump_num=jump_num,
            rotation=rotation,
            translation=translation,
            upper_water_distance=upper_water_distance,
            lower_water_distance=lower_water_distance,
            backbone_clash_keep_sidechains=backbone_clash_keep_sidechains,
            max_pass_score=max_pass_score,
            bin_width=bin_width,
            vdw_modifier=vdw_modifier,
            include_sc=include_sc,
            lig_res_num=lig_res_num,
            output_dirs=output_dirs,
            rename_water_to_tp3=rename_water_to_tp3,
            use_hbond_geometry_filter=use_hbond_geometry_filter,
            hbond_distance_min=hbond_distance_min,
            hbond_distance_max=hbond_distance_max,
            hbond_distance_ideal=hbond_distance_ideal,
            hbond_donor_angle_min=hbond_donor_angle_min,
            hbond_acceptor_angle_min=hbond_acceptor_angle_min,
            hbond_quality_min=hbond_quality_min,
            cluster_enabled=cluster_enabled,
            cluster_rmsd_cutoff=cluster_rmsd_cutoff,
        )
        pass_stats["pass_index"] = pass_idx
        pass_summaries.append(pass_stats)
        try:
            pass_df = pd.read_pickle(pkl_file_name)
            combined_df = pd.concat([combined_df, pass_df], ignore_index=True)
            print(f"Pass {pass_idx} DataFrame loaded and merged.")
        except Exception as e:
            print(f"Error loading pickle file for pass {pass_idx}: {e}")

    final_dir = aggregate_pass_score_repacked(
        pass_score_repacked_dirs,
        num_passes,
        output_base=output_base,
        final_dir_name="final_pass_score_repacked",
    )
    write_summary_report(output_base, array_index, "glycine", pass_summaries, final_dir)

    combined_pkl = "combined_" + pkl_file_name if pkl_file_name else "combined_results.pkl"
    combined_df.to_pickle(combined_pkl, protocol=4)
    print(f"\nAll passes completed. Combined results saved to {combined_pkl}")


if __name__ == "__main__":
    main(sys.argv[1:])
